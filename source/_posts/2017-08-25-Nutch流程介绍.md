---
title: Nutch流程介绍
date: 2017-08-25 09:48:25
categories:
- Nutch
tags:
- Nutch
---

# Generate
经历三类Job: select/partition/updatedb，最后打上`tag_generate`。

| 参数  | 说明 |
| ----- | -------- |
| crawl_db | crawldb存放目录 |
| segments_dir | Generate生成的segment存放目录 | 
| topN | Generate生成的单个Segment中可能包含的最大url数 |
| numFetchers | Generate中的 Partition Job 的Reduce数量（生成`crawl_generate`），会影响Fetch阶段的map数，可用于控制抓取的并发度 |
| minGenerateNum | 当 segments_dir 中未抓取的segment个数 <= minGenerateNum时，才生成新的generate |
| maxNumSegments | 一轮Generate中可能生成最大的Segment个数 |
| regex.filter.server.key | 值为`CRAWL-urlfilter`, 用于从config server获取 urlfilter-regex 规则|

## Select
- Input: `crawldb`
- Output: tmpdir
- Map: 
  1. 如果开启`filter`(默认true)，则根据配置的urlfilter插件进行过滤。
  2. 根据配置的`db.fetch.schedule.class`判断url是否应该加入select过程，默认判断其 FetchTime 是否到达当前时间。
  3. 根据`crawl.gen.delay`(默认是7天)，判断该url是否应该再次被 generate，等待被 fetch & update。
  4. ScoreFilter对url进行打分。
  5. 输出`<sortValue, selectEntry>`。
- Reduce:
  1. 如果生成的url数量超过 TopN，则新建一个 Segment，为每个输出的selectEntry标记一个 `segnum`。
  2. 如果配置了 urlnomalizer 插件，会对url进行normalize处理。
  3. `generate.count.mode`参数(默认是domain，crawl流程是host)决定url的计数maxCount，对应的host or domain最大的url数是`generate.max.count`(crawl流程是2000，默认是-1 unlimited)。超出会考虑是否能放入下一个 Segment。
  4. 输出`<sortValue, selectEntry>`。

## Partition
- Input: tmpdir
- Output: `crawl_generate`
- Map:
  * 输出`<selectEntry.url, selectEntry>`。
- Partition:
  * `partition.url.mode`参数决定(默认byHost，可设为byDomain/byIP)
- Reduce:
  * 输出`<url, crawlDatum>`。

## updatedb
- 更新 crawldb 的 genTime
- 删除old，将current重命名为old，状态更新后设为current。

# Fetch
开始Fetch打上`tag_fetching`，完成再打上`tag_fetched`。

| 参数  | 说明 |
| ----- | -------- |
| segments_dir | segment存放目录，从中选取未Fetch的第一个segment | 
| threads | 抓取的线程数，默认10，crawl为2000 |
| fetcher.timelimit.mins | 设置抓取的时间限制，超出时间结束抓取流程 |

- Input: `crawl_generate`, Generate流程中Partition的Reduce数量决定了Fetch的Map数量。
- Output: `content`、`crawl_fetch`
- Map:
  1. 构造一个 fetchQueues，`fetcher.queue.mode`(默认为byHost)设置每个queue用于抓取特定host，默认每个queue配备一个线程。
  2. `fetcher.server.delay`设置每个queue抓取间隔时间，默认为5s，Crawl流程为1s。
  3. `queueFeeder`用于将每个待抓取的url扔到对应的queue里。
  4. `fetchThread`用于抓取对应的queue，如果一个queue被其他线程消费则选择另外待抓取的queue（每个queue的inProgress表示其是否在被消费）。
  5. 具体发送请求抓取url是通过插件`protocol-xxx`完成的。


# Parse
- Input: `content`
- Output: `crawl_parse`、`parse_data`、`parse_text`（crawl流程中已去除）
- Map:
   1. 调用插件`parse-xxx`对`content`目录进行解析。
   2. 输出`<url, val>`。
- Reduce: (考虑去掉Reduce部分提高效率，但会造成很多文件生成)
   1. 输出url对应的一个值。

# UpdateDB
- Input: `crawl_fetch`、`crawl_parse`、`crawldb`
- Output: `crawldb`
默认`crawldb.url.filters`设为false，不进行过滤，news流程为true。
